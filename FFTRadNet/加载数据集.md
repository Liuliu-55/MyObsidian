[[FFTRadNet代码注释]]


## encoder.py
1.初始化
```python
class ra_encoder():

    def __init__(self, geometry, statistics,regression_layer = 2):

        self.geometry = geometry
        #"geometry":{

            #"ranges": [512,896,1],

            #"resolution": [0.201171875,0.2],

            #"size": 3

        self.statistics = statistics
        #"statistics":{

            #"input_mean":[],

            #"input_std":[],

            #"reg_mean":[],

            #"reg_std":[]

        #}

        self.regression_layer = regression_layer
        # 2

  

        self.INPUT_DIM = (geometry['ranges'][0],geometry['ranges'][1],geometry['ranges'][2])
        # [512,869,1]

        self.OUTPUT_DIM = (regression_layer + 1,self.INPUT_DIM[0] // 4 , self.INPUT_DIM[1] // 4 )
        # [3,128,224]
```

2.编码 ^aba8a1
```python
```


## dataset.py

1.初始化
```python
class RADIal(Dataset):

  

    def __init__(self, root_dir,statistics=None,encoder=None,difficult=False):

  

        self.root_dir = root_dir
        #数据集路径

        self.statistics = statistics
        #统计信息

        self.encoder = encoder
        #编码器
	    self.labels = pd.read_csv(os.path.join(root_dir,'labels.csv')).to_numpy()
	    # 读取label.csv
```

2.滤除difficult数据
```python	    
	    # Keeps only easy samples
        if(difficult==False):
        # difficult为0的是简单数据

            ids_filters=[]

            ids = np.where( self.labels[:, -1] == 0)[0]

            ids_filters.append(ids)

            ids_filters = np.unique(np.concatenate(ids_filters))

            self.labels = self.labels[ids_filters]
```

3.获得所有sample id信息
```python
        # Gather each input entries by their sample id

        self.unique_ids = np.unique(self.labels[:,0])
        # 去掉labels中id重复的元素

        # self.unique_ids = self.unique_ids[::50]
        # 每隔50抽取一个样本id

        self.label_dict = {}

        for i,ids in enumerate(self.unique_ids):

            sample_ids = np.where(self.labels[:,0]==ids)[0]
            # 该sample id的序号
            # 比如18这个sample id， 有序号为[0 1 2 3]的四条目标信息

            self.label_dict[ids]=sample_ids
            # {18：array{[0 1 2 3]},...}

        self.sample_keys = list(self.label_dict.keys())
        # 提取出self.label_dict的keys

  

        self.resize = Resize((256,224), interpolation=transform.InterpolationMode.NEAREST)

        self.crop = CenterCrop((512,448))
```

4.取数据
```python
    def __getitem__(self, index):

        # Get the sample id

        sample_id = self.sample_keys[index]
        # 序号index对应的sample id

  

        # From the sample id, retrieve all the labels ids

        entries_indexes = self.label_dict[sample_id]
        # sample id 对应的目标信息序号
        # {18:[0 1 2 3]}

  

        # Get the objects labels

        box_labels = self.labels[entries_indexes]
        # 根据目标信息序号entries_index去取目标的所有信息

  

        # Labels contains following parameters:

        # x1_pix    y1_pix  x2_pix  y2_pix  laser_X_m   laser_Y_m   laser_Z_m radar_X_m radar_Y_m   radar_R_m

  

        # format as following [Range, Angle, Doppler,laser_X_m,laser_Y_m,laser_Z_m,x1_pix,y1_pix,x2_pix ,y2_pix]

        box_labels = box_labels[:,[10,11,12,5,6,7,1,2,3,4]].astype(np.float32)
```

5.[[加载数据集#^aba8a1|编码]]
```python
# 编码box_labels

```
## dataloader.py

```python
Sequences = {'Validation':['RECORD@2020-11-22_12.49.56','RECORD@2020-11-22_12.11.49','RECORD@2020-11-22_12.28.47','RECORD@2020-11-21_14.25.06'],

            'Test':['RECORD@2020-11-22_12.45.05','RECORD@2020-11-22_12.25.47','RECORD@2020-11-22_12.03.47','RECORD@2020-11-22_12.54.38']}

def CreateDataLoaders(dataset,config=None,seed=0):
    elif(config['mode']=='sequence'):

        dict_index_to_keys = {s:i for i,s in enumerate(dataset.sample_keys)}
        # sample id的keys的序号
        # {18:0, 19:1, 115:2, ...}

  
# Validation的序列
        Val_indexes = []

        for seq in Sequences['Validation']:

            idx = np.where(dataset.labels[:,14]==seq)[0]

            Val_indexes.append(dataset.labels[idx,0])
		# sequence为validation的sample id
        Val_indexes = np.unique(np.concatenate(Val_indexes))
        # 去掉Val_indexes重复的元素

  
# Test的序列
        Test_indexes = []

        for seq in Sequences['Test']:

            idx = np.where(dataset.labels[:,14]==seq)[0]

            Test_indexes.append(dataset.labels[idx,0])
		# sequence为test的sample id
        Test_indexes = np.unique(np.concatenate(Test_indexes))
        # 去掉Test_indexes中重复的元素

  

        val_ids = [dict_index_to_keys[k] for k in Val_indexes]
        # sample id(unique) 为validation的序号

        test_ids = [dict_index_to_keys[k] for k in Test_indexes]
        # sample id(unique) 为test的序号

        train_ids = np.setdiff1d(np.arange(len(dataset)),np.concatenate([val_ids,test_ids]))
        # sample id(unique) 为Train的序号
  

        train_dataset = Subset(dataset,train_ids)

        val_dataset = Subset(dataset,val_ids)

        test_dataset = Subset(dataset,test_ids)

  

        print('===========  Dataset  ==================:')

        print('      Mode:', config['mode'])

        print('      Training:', len(train_dataset))

        print('      Validation:', len(val_dataset))

        print('      Test:', len(test_dataset))

        print('')
```